<!DOCTYPE html>
<!-- Added class to html tag for theme toggling -->
<html lang="en" class="h-full">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformer</title>
    <script src="https://cdn.tailwindcss.com"></script>

    <style>
        /* Inter font */
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');

        body {
            font-family: 'Inter', sans-serif;
        }

        /* --- LAYOUT & ANIMATION STYLES --- */

        /* This container holds all slides */
        #slide-container {
            position: relative;
            /* Allows slides to be layered on top of each other */
            flex: 1;
        }

        /* Base style for all slides */
        .slide {
            /* All slides are layered absolutely inside the container */
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;

            /* Animation properties */
            opacity: 0;
            /* Hidden by default */
            transition: opacity 0.4s ease-in-out;
            /* Smooth fade duration */
            pointer-events: none;
            /* Can't click on hidden slides */

            /* Layout: Vertically centers content by default */
            padding: 3rem 4rem;
            display: flex;
            flex-direction: column;
            justify-content: center;
            /* Vertically centers the content block */
        }

        /* The active slide is visible and clickable */
        .slide.active {
            opacity: 1;
            pointer-events: auto;
            z-index: 1;
            /* Brings active slide to the front */
        }

        /* Style for the fixed footer bar */
        .footer-bar {
            box-shadow: 0 -4px 6px -1px rgba(0, 0, 0, 0.05), 0 -2px 4px -1px rgba(0, 0, 0, 0.03);
        }

        .dark .footer-bar {
            /* Adjusted shadow for dark mode */
            box-shadow: 0 -4px 6px -1px rgba(0, 0, 0, 0.1), 0 -2px 4px -1px rgba(0, 0, 0, 0.08);
        }

        /* Updated Scrollbar for Horizontal Timeline */
        .timeline-container {
            width: 100%;
            overflow-x: auto;
            /* Enables horizontal scrolling */
            overflow-y: hidden;
            padding: 20px 0;
            /* Smooth scrolling */
            scroll-behavior: smooth;
        }

        /* Hide scrollbar for Chrome, Safari and Opera */
        .timeline-container::-webkit-scrollbar {
            height: 8px;
        }

        .timeline-container::-webkit-scrollbar-track {
            background: transparent;
        }

        .timeline-container::-webkit-scrollbar-thumb {
            background-color: rgba(156, 163, 175, 0.5);
            border-radius: 20px;
        }
    </style>
</head>
<!-- Added light and dark mode classes for body -->

<body class="bg-gray-50 dark:bg-slate-900 text-slate-900 dark:text-slate-100 min-h-screen flex flex-col">

    <!-- Main slide content area, flex-1 makes it grow to fill available space -->
    <div class="w-full flex-1 flex flex-col">
        <!-- Added light and dark mode classes for main container -->
        <div class="bg-white dark:bg-slate-800 flex-1 flex flex-col justify-between">

            <div id="slide-container">

                <!-- Slide 1: Title -->
                <div class="slide active" data-slide="1">
                    <h1 class="text-6xl sm:text-7xl font-bold mt-4">Inside Transformers with Attention is All you Need
                    </h1>
                    <p class="text-3xl mt-6 text-slate-600 dark:text-slate-300">By Dr. Wasim Ahmad Khan</p>
                    <div class="mt-16">
                        <p class="text-3xl">How do models like ChatGPT, Google Translate, and BERT <em>actually</em>
                            understand language so well?</p>
                        <p class="text-3xl mt-4">The answer for all of them is the <strong>Transformer</strong>.</p>
                    </div>
                </div>

                <!-- Slide 2: The Big Question -->
                <div class="slide text-center" data-slide="2">
                    <div class="flex flex-col items-center justify-center h-full">
                        <h2 class="text-7xl font-bold text-indigo-600 dark:text-indigo-400 leading-tight">
                            Why Attention is all you need?
                        </h2>
                    </div>
                </div>

                <!-- Slide 3: The Answer (Moved UP) -->
                <div class="slide text-center" data-slide="3">
                    <div class="flex flex-col items-center justify-center h-full">
                        <h2
                            class="text-6xl md:text-7xl font-bold text-indigo-600 dark:text-indigo-400 leading-tight mb-12">
                            That's Why Attention is all you need?
                        </h2>
                        <div class="space-y-6">
                            <p class="text-4xl text-slate-800 dark:text-slate-200 font-medium">You don't need
                                Recurrence.</p>
                            <p class="text-4xl text-slate-800 dark:text-slate-200 font-medium">You don't need
                                Convolutions.</p>
                            <p class="text-5xl text-indigo-600 dark:text-indigo-400 font-bold pt-8">Attention is all you
                                need.</p>
                        </div>
                    </div>
                </div>

                <!-- Slide 4: The Problem (Moved DOWN) -->
                <div class="slide" data-slide="4">
                    <div class="grid grid-cols-1 md:grid-cols-3 gap-12">
                        <div class="col-span-1 pr-8 border-r border-slate-200 dark:border-slate-600">
                            <h2 class="text-5xl font-bold text-indigo-600 dark:text-indigo-400">The Problem: Sequential
                                Models</h2>
                        </div>
                        <div class="col-span-2 pl-8">
                            <p class="text-3xl">Older models like <strong>RNNs (Recurrent Neural Networks)</strong>
                                processed text word-by-word, in order.</p>
                            <div class="mt-8 space-y-6">
                                <div>
                                    <h3 class="text-2xl font-semibold">Limitation 1: The Sequential Bottleneck</h3>
                                    <p class="text-2xl text-slate-600 dark:text-slate-300">You can't process word 5
                                        until you've processed word 4. This is <strong>slow</strong> and cannot be
                                        parallelized on modern GPUs.</p>
                                </div>
                                <div>
                                    <h3 class="text-2xl font-semibold">Limitation 2: Long-Range Dependency</h3>
                                    <p class="text-2xl text-slate-600 dark:text-slate-300">Information from the start of
                                        a long sentence gets "lost" by the time the model reaches the end.</p>
                                    <p class="mt-3 p-4 bg-slate-100 dark:bg-slate-700 rounded-md text-xl">e.g., "The
                                        <strong>cat</strong>, which chased the dog... all the way to the park, ...was
                                        <strong>fluffy</strong>."
                                    </p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Slide 5: Horizontal Timeline (Was 4) -->
                <div class="slide" data-slide="5">
                    <div class="flex flex-col h-full justify-center">
                        <h2 class="text-5xl font-bold text-indigo-600 dark:text-indigo-400 mb-8 text-center shrink-0">
                            Neural Network Timeline</h2>

                        <!-- Scrollable Wrapper -->
                        <div class="timeline-container relative">
                            <!-- Fixed Width Container to Ensure Scroll -->
                            <div class="min-w-[1200px] px-8 py-10 relative">

                                <!-- Central Horizontal Line -->
                                <div
                                    class="absolute top-1/2 left-0 w-full h-1 bg-slate-200 dark:bg-slate-600 transform -translate-y-1/2 rounded">
                                </div>

                                <!-- Grid for Items -->
                                <div class="grid grid-cols-6 gap-0 relative w-full h-[400px]">

                                    <!-- 1. 1943 ANN (TOP) -->
                                    <div
                                        class="flex flex-col items-center justify-end pb-[calc(50%-2px)] relative group h-full">
                                        <!-- Content -->
                                        <div
                                            class="mb-6 p-4 bg-white dark:bg-slate-700 shadow-xl rounded-xl text-center w-[90%] border border-slate-100 dark:border-slate-600 hover:-translate-y-2 transition-transform duration-300 z-30">
                                            <h3 class="text-2xl font-bold text-indigo-600 dark:text-indigo-400">ANN</h3>
                                            <p class="text-3xl font-extrabold text-slate-800 dark:text-slate-100 my-1">
                                                1943</p>
                                            <p
                                                class="text-sm text-slate-500 dark:text-slate-400 font-medium bg-slate-100 dark:bg-slate-800 py-1 px-2 rounded mt-2 inline-block">
                                                McCulloch & Pitts (1943)</p>
                                        </div>
                                        <!-- Connector -->
                                        <div
                                            class="absolute bottom-1/2 left-1/2 w-0.5 h-[calc(50%-20px)] bg-indigo-300 dark:bg-indigo-700 -z-10">
                                        </div>
                                        <!-- Dot -->
                                        <div
                                            class="absolute bottom-1/2 left-1/2 transform -translate-x-1/2 translate-y-1/2 w-6 h-6 bg-indigo-600 rounded-full border-4 border-white dark:border-slate-800 z-20 shadow-lg">
                                        </div>
                                    </div>

                                    <!-- 2. 1986 RNN (BOTTOM) -->
                                    <div
                                        class="flex flex-col items-center justify-start pt-[calc(50%-2px)] relative group h-full">
                                        <!-- Dot -->
                                        <div
                                            class="absolute top-1/2 left-1/2 transform -translate-x-1/2 -translate-y-1/2 w-6 h-6 bg-indigo-600 rounded-full border-4 border-white dark:border-slate-800 z-20 shadow-lg">
                                        </div>
                                        <!-- Connector -->
                                        <div
                                            class="absolute top-1/2 left-1/2 w-0.5 h-[calc(50%-20px)] bg-indigo-300 dark:bg-indigo-700 -z-10">
                                        </div>
                                        <!-- Content -->
                                        <div
                                            class="mt-6 p-4 bg-white dark:bg-slate-700 shadow-xl rounded-xl text-center w-[90%] border border-slate-100 dark:border-slate-600 hover:translate-y-2 transition-transform duration-300 z-30">
                                            <h3 class="text-2xl font-bold text-indigo-600 dark:text-indigo-400">RNN</h3>
                                            <p class="text-3xl font-extrabold text-slate-800 dark:text-slate-100 my-1">
                                                1986</p>
                                            <p
                                                class="text-sm text-slate-500 dark:text-slate-400 font-medium bg-slate-100 dark:bg-slate-800 py-1 px-2 rounded mt-2 inline-block">
                                                Rumelhart et al. (1986)</p>
                                        </div>
                                    </div>

                                    <!-- 3. 1997 LSTM (TOP) -->
                                    <div
                                        class="flex flex-col items-center justify-end pb-[calc(50%-2px)] relative group h-full">
                                        <!-- Content -->
                                        <div
                                            class="mb-6 p-4 bg-white dark:bg-slate-700 shadow-xl rounded-xl text-center w-[90%] border border-slate-100 dark:border-slate-600 hover:-translate-y-2 transition-transform duration-300 z-30">
                                            <h3 class="text-2xl font-bold text-indigo-600 dark:text-indigo-400">LSTM
                                            </h3>
                                            <p class="text-3xl font-extrabold text-slate-800 dark:text-slate-100 my-1">
                                                1997</p>
                                            <p
                                                class="text-sm text-slate-500 dark:text-slate-400 font-medium bg-slate-100 dark:bg-slate-800 py-1 px-2 rounded mt-2 inline-block">
                                                Hochreiter & Schmidhuber (1997)</p>
                                        </div>
                                        <div
                                            class="absolute bottom-1/2 left-1/2 w-0.5 h-[calc(50%-20px)] bg-indigo-300 dark:bg-indigo-700 -z-10">
                                        </div>
                                        <div
                                            class="absolute bottom-1/2 left-1/2 transform -translate-x-1/2 translate-y-1/2 w-6 h-6 bg-indigo-600 rounded-full border-4 border-white dark:border-slate-800 z-20 shadow-lg">
                                        </div>
                                    </div>

                                    <!-- 4. 1998 CNN (BOTTOM) -->
                                    <div
                                        class="flex flex-col items-center justify-start pt-[calc(50%-2px)] relative group h-full">
                                        <div
                                            class="absolute top-1/2 left-1/2 transform -translate-x-1/2 -translate-y-1/2 w-6 h-6 bg-indigo-600 rounded-full border-4 border-white dark:border-slate-800 z-20 shadow-lg">
                                        </div>
                                        <div
                                            class="absolute top-1/2 left-1/2 w-0.5 h-[calc(50%-20px)] bg-indigo-300 dark:bg-indigo-700 -z-10">
                                        </div>
                                        <div
                                            class="mt-6 p-4 bg-white dark:bg-slate-700 shadow-xl rounded-xl text-center w-[90%] border border-slate-100 dark:border-slate-600 hover:translate-y-2 transition-transform duration-300 z-30">
                                            <h3 class="text-2xl font-bold text-indigo-600 dark:text-indigo-400">CNN</h3>
                                            <p class="text-3xl font-extrabold text-slate-800 dark:text-slate-100 my-1">
                                                1998</p>
                                            <p
                                                class="text-sm text-slate-500 dark:text-slate-400 font-medium bg-slate-100 dark:bg-slate-800 py-1 px-2 rounded mt-2 inline-block">
                                                LeCun et al. (1998)</p>
                                        </div>
                                    </div>

                                    <!-- 5. 2014 GRU (TOP) -->
                                    <div
                                        class="flex flex-col items-center justify-end pb-[calc(50%-2px)] relative group h-full">
                                        <div
                                            class="mb-6 p-4 bg-white dark:bg-slate-700 shadow-xl rounded-xl text-center w-[90%] border border-slate-100 dark:border-slate-600 hover:-translate-y-2 transition-transform duration-300 z-30">
                                            <h3 class="text-2xl font-bold text-indigo-600 dark:text-indigo-400">GRU</h3>
                                            <p class="text-3xl font-extrabold text-slate-800 dark:text-slate-100 my-1">
                                                2014</p>
                                            <p
                                                class="text-sm text-slate-500 dark:text-slate-400 font-medium bg-slate-100 dark:bg-slate-800 py-1 px-2 rounded mt-2 inline-block">
                                                Cho et al. (2014)</p>
                                        </div>
                                        <div
                                            class="absolute bottom-1/2 left-1/2 w-0.5 h-[calc(50%-20px)] bg-indigo-300 dark:bg-indigo-700 -z-10">
                                        </div>
                                        <div
                                            class="absolute bottom-1/2 left-1/2 transform -translate-x-1/2 translate-y-1/2 w-6 h-6 bg-indigo-600 rounded-full border-4 border-white dark:border-slate-800 z-20 shadow-lg">
                                        </div>
                                    </div>

                                    <!-- 6. 2017 Transformers (BOTTOM) -->
                                    <div
                                        class="flex flex-col items-center justify-start pt-[calc(50%-2px)] relative group h-full">
                                        <!-- Special Transformer Dot -->
                                        <div
                                            class="absolute top-1/2 left-1/2 transform -translate-x-1/2 -translate-y-1/2 w-8 h-8 bg-indigo-600 rounded-full border-4 border-white dark:border-slate-800 ring-4 ring-indigo-300 dark:ring-indigo-800 z-20 shadow-lg animate-pulse">
                                        </div>
                                        <div
                                            class="absolute top-1/2 left-1/2 w-0.5 h-[calc(50%-20px)] bg-indigo-300 dark:bg-indigo-700 -z-10">
                                        </div>
                                        <div
                                            class="mt-6 p-4 bg-white dark:bg-slate-700 shadow-2xl rounded-xl text-center w-[90%] border-2 border-indigo-500 dark:border-indigo-400 hover:translate-y-2 transition-transform duration-300 z-30">
                                            <h3 class="text-2xl font-bold text-indigo-600 dark:text-indigo-400">
                                                Transformers</h3>
                                            <p class="text-3xl font-extrabold text-slate-800 dark:text-slate-100 my-1">
                                                2017</p>
                                            <p
                                                class="text-sm text-slate-500 dark:text-slate-400 font-medium bg-slate-100 dark:bg-slate-800 py-1 px-2 rounded mt-2 inline-block">
                                                Vaswani et al. (2017)</p>
                                        </div>
                                    </div>

                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Slide 6: Introduction Video (Was 5) -->
                <div class="slide" data-slide="6">
                    <div class="flex flex-col items-center justify-center h-full">
                        <h2 class="text-4xl font-bold text-indigo-600 dark:text-indigo-400 mb-6">Introduction</h2>
                        <div class="w-full max-w-5xl aspect-video bg-black rounded-xl overflow-hidden shadow-2xl">
                            <video controls class="w-full h-full object-contain">
                                <source src="videos/Introduction.mp4" type="video/mp4">
                                Your browser does not support the video tag.
                            </video>
                        </div>
                    </div>
                </div>

                <!-- Slide 7: Full Architecture (Was 6) -->
                <div class="slide items-center" data-slide="7">
                    <div class="w-full">
                        <h2 class="text-5xl font-bold text-indigo-600 dark:text-indigo-400 mb-6">The Full Architecture
                            (Overview)</h2>
                        <div class="flex flex-col md:flex-row gap-6 mt-6">
                            <div class="flex-1">
                                <img src="images/model_arci.png" alt="Transformer Architecture Diagram"
                                    class="rounded-lg shadow-xl w-full h-auto object-contain max-h-[600px]"
                                    onerror="this.onerror=null; this.src='https://placehold.co/600x400?text=Transformer+Architecture+Diagram';">
                            </div>
                            <div class="flex-1 space-y-6 my-auto">
                                <p class="text-3xl">This is the complete model from the paper. It uses a stack of
                                    Encoders and Decoders.</p>
                                <div class="p-4 bg-slate-100 dark:bg-slate-700 rounded-lg">
                                    <h3 class="text-2xl font-semibold text-indigo-600 dark:text-indigo-400">The Encoder
                                        (Left Side)</h3>
                                    <p class="mt-2 text-xl">Its job is to <strong>"understand"</strong> the input
                                        sentence.</p>
                                </div>
                                <div class="p-4 bg-slate-100 dark:bg-slate-700 rounded-lg">
                                    <h3 class="text-2xl font-semibold text-indigo-600 dark:text-indigo-400">The Decoder
                                        (Right Side)</h3>
                                    <p class="mt-2 text-xl">Its job is to <strong>"generate"</strong> the output
                                        sentence.</p>
                                </div>
                                <p class="text-2xl">Now, let's break down the components inside each of these blocks.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Slide 8: Input Embeddings (Was 7) -->
                <div class="slide" data-slide="8">
                    <div class="grid grid-cols-1 md:grid-cols-3 gap-12">
                        <div class="col-span-1 pr-8 border-r border-slate-200 dark:border-slate-600">
                            <h2 class="text-5xl font-bold text-indigo-600 dark:text-indigo-400">Input Embeddings</h2>
                        </div>
                        <div class="col-span-2 pl-8">
                            <p class="text-3xl">Before anything else, we must convert words into numbers the model can
                                understand.</p>
                            <div class="mt-8 space-y-6">
                                <div class="p-4 bg-slate-100 dark:bg-slate-700 rounded-lg">
                                    <p class="text-2xl">Words are converted into high-dimensional numbers (vectors).</p>
                                </div>
                                <p class="text-3xl">Think of it like giving each word a unique ID badge, but one that
                                    also has coordinates showing how similar it is to other words.</p>
                                <ul
                                    class="list-disc list-inside text-2xl text-slate-600 dark:text-slate-300 ml-4 space-y-2">
                                    <li>"King" and "Queen" would be close together.</li>
                                    <li>"King" and "Apple" would be far apart.</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Slide 9: Positional Encoding (Was 8) -->
                <div class="slide" data-slide="9">
                    <div class="grid grid-cols-1 md:grid-cols-3 gap-12">
                        <div class="col-span-1 pr-8 border-r border-slate-200 dark:border-slate-600">
                            <h2 class="text-5xl font-bold text-indigo-600 dark:text-indigo-400">Positional Encoding</h2>
                        </div>
                        <div class="col-span-2 pl-8">
                            <p class="text-3xl"><strong>A New Problem:</strong> Self-Attention (which we'll see) looks
                                at all words at once. It has no idea about word order.</p>
                            <p class="text-2xl mt-6 text-slate-600 dark:text-slate-300">To the model, "The man bites the
                                dog" and "The dog bites the man" look identical!</p>
                            <div class="mt-8">
                                <h3 class="text-2xl font-semibold">The Solution:</h3>
                                <p class="text-2xl text-slate-600 dark:text-slate-300 mt-2">We create a unique
                                    "Positional Encoding" vector for each position (1st, 2nd, 3rd...).</p>
                                <p class="mt-3 text-2xl text-slate-600 dark:text-slate-300">This vector is
                                    <strong>added</strong> to the word's embedding, giving the model a signal for where
                                    each word is in the sequence.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Slide 10: Positional Encoding Video (Was 9) -->
                <div class="slide" data-slide="10">
                    <div class="flex flex-col items-center justify-center h-full">
                        <h2 class="text-4xl font-bold text-indigo-600 dark:text-indigo-400 mb-6">Positional Encoding
                            Visualization</h2>
                        <div class="w-full max-w-5xl aspect-video bg-black rounded-xl overflow-hidden shadow-2xl">
                            <video controls class="w-full h-full object-contain">
                                <source src="videos/pos_encoding.mp4" type="video/mp4">
                                Your browser does not support the video tag.
                            </video>
                        </div>
                    </div>
                </div>

                <!-- Slide 11: The Idea of Self-Attention (Was 10) -->
                <div class="slide" data-slide="11">
                    <div class="grid grid-cols-1 md:grid-cols-3 gap-12">
                        <div class="col-span-1 pr-8 border-r border-slate-200 dark:border-slate-600">
                            <h2 class="text-5xl font-bold text-indigo-600 dark:text-indigo-400">The Idea of
                                Self-Attention</h2>
                        </div>
                        <div class="col-span-2 pl-8">
                            <p class="text-3xl">The 2017 paper <strong
                                    class="text-indigo-500 dark:text-indigo-300">"Attention Is All You Need"</strong>
                                got rid of recurrence entirely.</p>
                            <p class="text-3xl mt-4">The solution is <strong>Self-Attention</strong>: A mechanism that
                                lets the model look at <em>all</em> other words in the sentence <em>at the same
                                    time</em> when processing a single word.</p>
                            <div class="mt-8">
                                <h3 class="text-2xl font-semibold">Analogy:</h3>
                                <p class="mt-3 p-4 bg-slate-100 dark:bg-slate-700 rounded-md text-2xl">"The
                                    <strong>animal</strong> didn't cross the street because <strong>it</strong> was too
                                    tired."
                                </p>
                                <p class="mt-3 text-2xl text-slate-600 dark:text-slate-300">Attention instantly links
                                    <strong>"it"</strong> back to <strong>"animal"</strong>, no matter the distance.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Slide 12: Self-Attention Video (Was 11) -->
                <div class="slide" data-slide="12">
                    <div class="flex flex-col items-center justify-center h-full">
                        <h2 class="text-4xl font-bold text-indigo-600 dark:text-indigo-400 mb-6">Self-Attention
                            Visualization</h2>
                        <div class="w-full max-w-5xl aspect-video bg-black rounded-xl overflow-hidden shadow-2xl">
                            <video controls class="w-full h-full object-contain">
                                <source src="videos/self_attention.mp4" type="video/mp4">
                                Your browser does not support the video tag.
                            </video>
                        </div>
                    </div>
                </div>

                <!-- Slide 13: Dot-Product Attention (Was 12) -->
                <div class="slide items-center" data-slide="13">
                    <div class="w-full">
                        <h2 class="text-5xl font-bold text-indigo-600 dark:text-indigo-400 mb-6">How Self-Attention
                            Works</h2>
                        <div class="flex flex-col md:flex-row gap-8 mt-6">
                            <div class="flex-1 space-y-4">
                                <p class="text-2xl">For each word, we create three vectors:</p>
                                <ul
                                    class="list-disc list-inside space-y-3 mt-4 text-2xl text-slate-600 dark:text-slate-300 ml-4">
                                    <li><strong>Query (Q):</strong> "What I am looking for."</li>
                                    <li><strong>Key (K):</strong> "What I contain." (A label)</li>
                                    <li><strong>Value (V):</strong> "What I actually am." (The content)</li>
                                </ul>
                                <p class="text-2xl mt-6"><strong>The Process: (Like a library)</strong></p>
                                <ol
                                    class="list-decimal list-inside space-y-3 mt-2 text-2xl text-slate-600 dark:text-slate-300 ml-4">
                                    <li><strong>Score:</strong> Compare your Q (search query) to every word's K (book
                                        title).</li>
                                    <li><strong>Scale:</strong> (A math step to keep numbers stable).</li>
                                    <li><strong>Weights (Softmax):</strong> Turn scores into probabilities.</li>
                                    <li><strong>Output:</strong> Combine all V's (book contents) based on these weights.
                                    </li>
                                </ol>
                            </div>
                            <div class="flex-1 flex flex-col items-center justify-center">
                                <img src="images/dot_product.png" alt="Diagram of Scaled Dot-Product Attention"
                                    class="rounded-lg shadow-md w-full object-contain max-h-[550px]"
                                    onerror="this.onerror=null; this.src='https://placehold.co/600x400?text=Dot-Product+Attention';">
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Slide 14: Multi-Head Attention (Was 13) -->
                <div class="slide items-center" data-slide="14">
                    <div class="w-full">
                        <h2 class="text-5xl font-bold text-indigo-600 dark:text-indigo-400 mb-6">Multi-Head Attention
                        </h2>
                        <div class="flex flex-col md:flex-row gap-8 mt-6">
                            <div class="flex-1 space-y-5">
                                <p class="text-3xl">Why use just one set of Q, K, V?</p>
                                <p class="text-2xl mt-4 text-slate-600 dark:text-slate-300">A word can have multiple
                                    relationships. In "The cat sat on the mat":</p>
                                <ul class="list-disc list-inside text-2xl text-slate-600 dark:text-slate-300 mt-2 ml-4">
                                    <li>"sat" relates to "cat" (who sat)</li>
                                    <li>"sat" relates to "mat" (where it sat)</li>
                                </ul>
                                <div class="mt-6">
                                    <h3 class="text-2xl font-semibold">Solution: Multi-Head Attention (e.g., 8 heads)
                                    </h3>
                                    <p class="text-2xl text-slate-600 dark:text-slate-300 mt-2">It's like having 8
                                        different "experts" (heads) look at the sentence in parallel. Each head learns a
                                        different kind of relationship.</p>
                                    <p class="mt-3 text-2xl text-slate-600 dark:text-slate-300">The results from all
                                        heads are combined to get a final, rich representation.</p>
                                </div>
                            </div>
                            <div class="flex-1 flex flex-col items-center justify-center">
                                <img src="images/Multi_head.png" alt="Multi-Head Attention Diagram"
                                    class="rounded-lg shadow-md w-full object-contain max-h-[550px]"
                                    onerror="this.onerror=null; this.src='https://placehold.co/600x400?text=Multi-Head+Attention';">
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Slide 15: Feed-Forward Network (Was 14) -->
                <div class="slide" data-slide="15">
                    <div class="grid grid-cols-1 md:grid-cols-3 gap-12">
                        <div class="col-span-1 pr-8 border-r border-slate-200 dark:border-slate-600">
                            <h2 class="text-5xl font-bold text-indigo-600 dark:text-indigo-400">Feed-Forward Network
                            </h2>
                        </div>
                        <div class="col-span-2 pl-8">
                            <p class="text-3xl">After the attention step, the model needs to "process" the information
                                it has gathered.</p>
                            <div class="mt-8 space-y-6">
                                <div class="p-5 bg-slate-100 dark:bg-slate-700 rounded-lg">
                                    <p class="text-2xl">After attention, each word's new vector (rich with context) is
                                        passed through a simple, standard neural network.</p>
                                </div>
                                <p class="text-3xl">This is like the chef mixing the ingredients after gathering them.
                                </p>
                                <p class="text-2xl text-slate-600 dark:text-slate-300">This step gives the model more
                                    power and allows it to learn more complex patterns from the context-rich vectors.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Slide 16: NN Video (Was 15) -->
                <div class="slide" data-slide="16">
                    <div class="flex flex-col items-center justify-center h-full">
                        <h2 class="text-4xl font-bold text-indigo-600 dark:text-indigo-400 mb-6">Neural Network
                            Visualization</h2>
                        <div class="w-full max-w-5xl aspect-video bg-black rounded-xl overflow-hidden shadow-2xl">
                            <video controls class="w-full h-full object-contain">
                                <source src="videos/nn.mp4" type="video/mp4">
                                Your browser does not support the video tag.
                            </video>
                        </div>
                    </div>
                </div>

                <!-- Slide 17: Residuals & Layer Norm (Was 16) -->
                <div class="slide" data-slide="17">
                    <div class="w-full">
                        <h2 class="text-5xl font-bold text-indigo-600 dark:text-indigo-400">Residuals & Layer Norm</h2>
                        <p class="text-3xl mt-6 text-slate-600 dark:text-slate-300">These two components are the "glue"
                            that holds the model together and allows it to be trained successfully.</p>

                        <div class="mt-12 grid grid-cols-1 md:grid-cols-2 gap-8">
                            <div class="p-6 bg-slate-100 dark:bg-slate-700 rounded-lg">
                                <h3 class="text-2xl font-semibold">Residual Connections</h3>
                                <p class="mt-3 text-xl">This is a "shortcut." We add the *original* input vector to the
                                    *output* of the attention/FFN layer.</p>
                                <p class="mt-3 text-xl">This prevents information from being lost as it goes through
                                    many layers.</p>
                            </div>
                            <div class="p-6 bg-slate-100 dark:bg-slate-700 rounded-lg">
                                <h3 class="text-2xl font-semibold">Layer Normalization</h3>
                                <p class="mt-3 text-xl">This keeps the numbers (vectors) stable and under control during
                                    training.</p>
                                <p class="mt-3 text-xl">Think of it like safety rails on a highway, preventing the
                                    model's calculations from "going off track."</p>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Slide 18: Residual Video (Was 17) -->
                <div class="slide" data-slide="18">
                    <div class="flex flex-col items-center justify-center h-full">
                        <h2 class="text-4xl font-bold text-indigo-600 dark:text-indigo-400 mb-6">Residual Connection
                            Visualization</h2>
                        <div class="w-full max-w-5xl aspect-video bg-black rounded-xl overflow-hidden shadow-2xl">
                            <video controls class="w-full h-full object-contain">
                                <source src="videos/residual.mp4" type="video/mp4">
                                Your browser does not support the video tag.
                            </video>
                        </div>
                    </div>
                </div>

                <!-- Slide 19: Step-by-Step (Was 18) -->
                <div class="slide" data-slide="19">
                    <div class="grid grid-cols-1 md:grid-cols-3 gap-12">
                        <div class="col-span-1 pr-8 border-r border-slate-200 dark:border-slate-600">
                            <h2 class="text-5xl font-bold text-indigo-600 dark:text-indigo-400">Step-by-Step Example:
                            </h2>
                            <p class="text-3xl mt-4">Translating "I love cats"</p>
                        </div>
                        <div class="col-span-2 pl-8">
                            <ol class="list-decimal list-inside text-3xl space-y-5 text-slate-600 dark:text-slate-300">
                                <li><strong>Input:</strong> "I love cats" (Converted to Embeddings).</li>
                                <li><strong>Position Info:</strong> Add vectors for 1st, 2nd, 3rd position.</li>
                                <li><strong>Encoder:</strong> The stack of Encoder layers processes this, creating a
                                    rich understanding of the sentence.</li>
                                <li><strong>Decoder Starts:</strong> The Decoder begins generating the output, word by
                                    word.</li>
                                <li><strong>Decoder (Word 1):</strong> It pays attention to "I" and generates "Je".</li>
                                <li><strong>Decoder (Word 2):</strong> It pays attention to "love" and generates "aime".
                                </li>
                                <li><strong>Decoder (Word 3):</strong> It pays attention to "cats" and generates "les
                                    chats".</li>
                                <li><strong>Result:</strong> "Je aime les chats"</li>
                            </ol>
                        </div>
                    </div>
                </div>

                <!-- Slide 20: Why It Won (Was 19) -->
                <div class="slide" data-slide="20">
                    <div class="grid grid-cols-1 md:grid-cols-3 gap-12">
                        <div class="col-span-1 pr-8 border-r border-slate-200 dark:border-slate-600">
                            <h2 class="text-5xl font-bold text-indigo-600 dark:text-indigo-400">7. Why It Won: The
                                Impact</h2>
                        </div>
                        <div class="col-span-2 pl-8">
                            <ul class="list-disc list-inside text-3xl space-y-8">
                                <li>
                                    <strong>Massive Parallelization:</strong>
                                    <span class="block text-2xl text-slate-600 dark:text-slate-300 mt-2">Calculations
                                        can be done all at once (matrix math), not word-by-word. This is perfect for
                                        GPUs and much faster to train.</span>
                                </li>
                                <li>
                                    <strong>Superior Context:</strong>
                                    <span class="block text-2xl text-slate-600 dark:text-slate-300 mt-2">The attention
                                        mechanism provides a direct path between any two words, solving the long-range
                                        dependency problem.</span>
                                </li>
                                <li>
                                    <strong>Foundation for Modern AI:</strong>
                                    <span class="block text-2xl text-slate-600 dark:text-slate-300 mt-2">
                                        <strong>BERT</strong> (Encoders), <strong>GPT</strong> (Decoders), and
                                        <strong>T5</strong> (Full Model) are all built on this architecture.
                                    </span>
                                </li>
                            </ul>
                        </div>
                    </div>
                </div>

                <!-- Slide 21: Conclusion (Was 20) -->
                <div class="slide" data-slide="21">
                    <div class="w-full">
                        <h2 class="text-5xl font-bold text-indigo-600 dark:text-indigo-400">Conclusion & Q&A</h2>

                        <div class="mt-16 grid grid-cols-1 md:grid-cols-2 gap-12 items-center">
                            <div>
                                <p class="text-3xl">Summary of Key Points:</p>
                                <ul
                                    class="list-disc list-inside text-2xl space-y-4 mt-6 text-slate-600 dark:text-slate-300 ml-4">
                                    <li>Transformers replaced <strong>recurrence (RNNs)</strong> with
                                        <strong>attention</strong>.
                                    </li>
                                    <li>The core idea is <strong>Self-Attention</strong> (Query, Key, Value).</li>
                                    <li><strong>Multi-Head Attention</strong> learns different relationships in
                                        parallel.</li>
                                    <li><strong>Positional Encoding</strong> adds the word order information back.</li>
                                    <li>This design is <strong>faster (parallel)</strong> and <strong>smarter
                                            (context)</strong>.</li>
                                </ul>
                            </div>
                            <div class="text-center">
                                <h2 class="text-6xl font-bold">Thank You.<br>Questions?</h2>
                            </div>
                        </div>
                    </div>
                </div>

            </div>
            <!-- End of slide container -->

        </div>
    </div>

    <!-- Sticky footer bar for navigation -->
    <div
        class="sticky bottom-0 bg-white dark:bg-slate-800 border-t border-slate-200 dark:border-slate-700 px-8 py-4 flex items-center justify-between footer-bar">
        <button id="prevBtn"
            class="bg-indigo-600 hover:bg-indigo-700 text-white font-medium py-2 px-5 rounded-lg transition duration-200 disabled:opacity-50 disabled:cursor-not-allowed">
            Previous
        </button>

        <div id="slideCounter" class="text-sm font-medium text-slate-600 dark:text-slate-300">
            Slide 1 / 21
        </div>

        <button id="nextBtn"
            class="bg-indigo-600 hover:bg-indigo-700 text-white font-medium py-2 px-5 rounded-lg transition duration-200 disabled:opacity-50 disabled:cursor-not-allowed">
            Next
        </button>
    </div>

    <script>
        // --- Theme Logic (Auto-detect) ---
        const htmlEl = document.documentElement;

        function setTheme(theme) {
            if (theme === 'dark') {
                htmlEl.classList.add('dark');
                localStorage.setItem('theme', 'dark');
            } else {
                htmlEl.classList.remove('dark');
                localStorage.setItem('theme', 'light');
            }
        }

        const storedTheme = localStorage.getItem('theme');
        const preferredTheme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
        setTheme(storedTheme || preferredTheme);

        // JavaScript for slide navigation
        let currentSlide = 0;
        const slides = document.querySelectorAll('.slide');
        const totalSlides = slides.length;

        const prevBtn = document.getElementById('prevBtn');
        const nextBtn = document.getElementById('nextBtn');
        const slideCounter = document.getElementById('slideCounter');

        function showSlide(n) {
            if (slides[currentSlide]) {
                slides[currentSlide].classList.remove('active');

                // Pause video if current slide has one
                const video = slides[currentSlide].querySelector('video');
                if (video) {
                    video.pause();
                }
            }

            if (n >= totalSlides) {
                currentSlide = totalSlides - 1;
            } else if (n < 0) {
                currentSlide = 0;
            } else {
                currentSlide = n;
            }

            if (slides[currentSlide]) {
                slides[currentSlide].classList.add('active');
            }

            slideCounter.textContent = `Slide ${currentSlide + 1} / ${totalSlides}`;

            prevBtn.disabled = (currentSlide === 0);
            nextBtn.disabled = (currentSlide === totalSlides - 1);
        }

        prevBtn.addEventListener('click', () => {
            showSlide(currentSlide - 1);
        });

        nextBtn.addEventListener('click', () => {
            showSlide(currentSlide + 1);
        });

        document.addEventListener('keydown', (e) => {
            if (e.key === 'ArrowRight' && currentSlide < totalSlides - 1) {
                showSlide(currentSlide + 1);
            } else if (e.key === 'ArrowLeft' && currentSlide > 0) {
                showSlide(currentSlide - 1);
            }
        });
    </script>
</body>

</html>